[一个清楚的视频（B站）](https://www.bilibili.com/video/BV1C3dqYxE3q/?spm_id_from=333.337.search-card.all.click&vd_source=a503248b608b8da9614b6dd7eb24901d)
1. 把输入的内容通过词嵌入的方式转换成词向量矩阵（每一个词一个向量）
2. 加入位置信息
3. 多头注意力的处理（见下）
4. 残差网络和归一化处理（为了解决梯度消失、并且让分布更稳定而做的优化）

注意力机制

QKV向量
（整个词向量矩阵分别和WQKV矩阵（可以训练得到）相乘，得到各自的qkv，维度和原来相同，之后q1分别和k1,k2,k3,k4点积，得到和每一个词的相似度，然后再各自和v向量相乘再相加，得到转换后的a1，得到包含位置信息和所有的上下文信息，且是从第一个词的视角来看的的转换后的向量）

多头注意力机制，拼接

单头注意力就是先Q,K相乘，得到相似度，（缩放、严码、SoftMax），然后再和V相乘
多头注意力机制就是先把QKV拆分成多组，依次经过单头注意力机制运算后，再拼接，最后还有一次线性变换（不是简单拼接）


编码器和解码器
举例（翻译）：

比如：你输入英文句子

> "I love you."

编码器的工作是：

- 先把每个词（"I"、"love"、"you"）变成向量。
    
- 用自注意力（Self-Attention）让每个词了解整个句子的意思。
    
- 把整个句子"压缩"成一堆向量（叫**上下文表示**），  
    比如说，"love" 的向量里也包含了 "I" 和 "you" 的信息。
    

总结一句话：

> **编码器就是理解原文，把原文变成一个全局理解后的表示。**

---

解码器（Decoder）的作用：

👉 **根据编码器理解的内容，逐字逐句生成目标语言。**

比如你想翻译成中文，解码器的过程是：

- 一开始，解码器啥都不知道，只知道要开始翻译。
    
- 它先根据编码器的输出，生成第一个字，比如 "我"。
    
- 再用已经生成的"我"加上编码器的信息，继续预测下一个字，比如 "爱"。
    
- 不断循环，直到生成完整的句子"我爱你"。
    

注意⚡️：

- 解码器内部也有自注意力，但只看**自己已生成的内容**（防止作弊看到未来）。
    
- 并且还有**交互注意力**，即对编码器的输出做注意力（理解原文内容）。
    

总结一句话：

> **解码器就是边看已有翻译，边参考原文理解，逐步生成目标语言。**

